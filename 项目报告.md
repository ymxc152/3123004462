# 论文查重系统 - 项目报告

| 项目信息 | 详情 |
|---------|------|
| **课程** | [软件工程](https://edu.cnblogs.com/campus/gdgy/Class34Grade23ComputerScience?filter=homework) |
| **作业要求** | [个人编程作业](https://edu.cnblogs.com/campus/gdgy/Class34Grade23ComputerScience/homework/13477) |
| **项目目标** | 实现一个论文查重程序，规范软件开发流程，熟悉Github进行源代码管理和学习软件测试 |
| **GitHub仓库** | [https://github.com/ymxc152/3123004462](https://github.com/ymxc152/3123004462) |

---

## 1. PSP表格

| PSP2.1 | Personal Software Process Stages | 预估耗时（分钟） | 实际耗时（分钟） |
|:---|:---|:---:|:---:|
| **Planning** | **计划** | **10** | **8** |
| · Estimate | · 估计这个任务需要多少时间 | 10 | 8 |
| **Development** | **开发** | **325** | **283** |
| · Analysis | · 需求分析 (包括学习新技术) | 60 | 45 |
| · Design Spec | · 生成设计文档 | 30 | 35 |
| · Design Review | · 设计复审 | 15 | 10 |
| · Coding Standard | · 代码规范 (为目前的开发制定合适的规范) | 10 | 15 |
| · Design | · 具体设计 | 20 | 30 |
| · Coding | · 具体编码 | 150 | 120 |
| · Code Review | · 代码复审 | 30 | 20 |
| · Test | · 测试 (自我测试, 修改代码, 提交修改) | 10 | 8 |
| **Reporting** | **报告** | **60** | **68** |
| · Test Repor | · 测试报告 | 35 | 40 |
| · Size Measurement | · 计算工作量 | 15 | 8 |
| · Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划 | 10 | 15 |
| | **· 合计** | **395** | **354** |

---

## 2. 项目概述

### 2.1 项目简介
本项目是一个基于Python开发的论文查重系统，采用模块化设计，支持命令行和Web界面两种使用方式。系统通过多种相似度算法计算两篇论文的重复率。

### 2.2 技术特点
- **改进相似度算法**：使用改进的Jaccard相似度，考虑词频权重，提高准确度
- **中文文本处理**：使用jieba进行中文分词，支持中文停用词过滤
- **模块化架构**：清晰的模块划分，便于测试和维护
- **完善的异常处理**：处理各种边界情况和异常情况
- **自动化测试**：单元测试、批量测试和综合测试的完整测试体系
- **作业规范符合**：严格按照作业要求实现输入输出格式和性能要求

### 2.3 作业规范实现
- **输入输出规范**：严格按照命令行参数传递绝对路径，输出文件只包含相似度数值
- **性能要求**：5秒内完成计算，内存使用不超过2048MB
- **错误处理**：完善的异常处理机制，提供清晰的错误提示
- **安全要求**：不连接网络，不读写其他文件，不影响系统安全

---

## 3. 模块接口与设计

### 3.1 整体架构设计

本项目采用**模块化设计**，将论文查重系统分解为多个独立且功能明确的模块，便于维护、测试和扩展。

```
论文查重系统
├── main.py                    # 主入口程序
├── src/                       # 核心算法模块
│   ├── algorithm.py          # 相似度计算核心模块
│   ├── text_processor.py     # 文本预处理模块
│   ├── similarity_calculator.py # 相似度计算算法模块
│   ├── file_utils.py         # 文件处理工具模块
│   ├── result_formatter.py   # 结果格式化模块
│   └── report_generator.py   # 报告生成模块
├── tests/                     # 测试模块
│   ├── test_algorithm.py     # 单元测试
│   └── test_batch.py         # 批量测试
├── web/                      # Web界面模块
│   └── app.py               # Flask应用
├── data/                     # 测试数据目录
└── output/                   # 输出结果目录
```

### 3.2 核心算法流程图

#### 3.2.1 主程序流程图

```mermaid
flowchart TD
    A[程序启动] --> B[解析命令行参数]
    B --> C{参数数量正确?}
    C -->|否| D[显示使用说明并退出]
    C -->|是| E[检查文件是否存在]
    E --> F{文件存在?}
    F -->|否| G[显示错误信息并退出]
    F -->|是| H[读取文件内容]
    H --> I{文件读取成功?}
    I -->|否| J[显示读取错误并退出]
    I -->|是| K[调用相似度计算]
    K --> L[写入结果文件]
    L --> M{写入成功?}
    M -->|否| N[显示写入错误并退出]
    M -->|是| O[显示成功信息]
    O --> P[程序结束]
```

#### 3.2.2 文本相似度计算流程

```mermaid
flowchart TD
    A[输入原文和抄袭版文本] --> B[文本预处理模块]
    B --> C[清洗文本]
    C --> D[分词处理]
    D --> E[去除停用词]
    E --> F[向量化处理]
    F --> G[改进Jaccard相似度计算]
    G --> H[基础Jaccard相似度]
    G --> I[加权相似度计算]
    H --> J[相似度融合]
    I --> J
    J --> K[输出相似度结果]
```

#### 3.2.3 文本预处理流程

```mermaid
flowchart TD
    A[原始文本输入] --> B[去除标点符号模块]
    B --> C[统一大小写模块]
    C --> D[规范化空格模块]
    D --> E[过滤停用词模块]
    E --> F[标准化文本输出]
```

### 3.3 核心模块接口说明

#### 3.3.1 相似度计算核心模块 (`src/algorithm.py`)

**主函数接口：**
```python
def calculate_similarity(text1, text2):
    """
    计算两段文本的相似度（主入口函数）
    
    Args:
        text1 (str): 原文文本
        text2 (str): 抄袭版文本
    
    Returns:
        float: 相似度值 (0.0-1.0)
        
    Raises:
        TypeError: 输入类型错误
        ValueError: 文本内容无效
    """
```

**功能特点：**
- 整合文本预处理和相似度计算
- 使用改进的Jaccard相似度算法，考虑词频权重
- 返回0.0-1.0范围的相似度值
- 处理边界情况和异常输入

#### 3.3.2 文本预处理模块 (`src/text_processor.py`)

**核心函数接口：**
```python
def preprocess_text(text):
    """完整的文本预处理流程"""
    
def clean_text(text):
    """清洗文本，去除特殊字符和多余空格"""
    
def tokenize_text(text):
    """对文本进行分词处理"""
    
def remove_stop_words(words):
    """去除停用词"""
    
def vectorize_text(text):
    """将文本转换为向量表示（词频向量）"""
```

**功能特点：**
- 支持中文文本处理
- 使用jieba进行中文分词
- 内置中文停用词库
- 文本清洗和标准化

#### 3.3.3 相似度计算算法模块 (`src/similarity_calculator.py`)

**核心函数接口：**
```python
def cosine_similarity(vec1, vec2):
    """计算余弦相似度"""
    
def jaccard_similarity(set1, set2):
    """计算Jaccard相似度"""
    
def enhanced_jaccard_similarity(words1, words2):
    """改进的Jaccard相似度计算，考虑词频权重"""
    
def word_overlap_similarity(words1, words2):
    """计算词汇重叠相似度"""
```

**算法特点：**
- 余弦相似度：基于向量夹角计算，使用NumPy优化
- Jaccard相似度：基于集合交集计算
- 改进Jaccard相似度：结合词频权重，给重要词汇更高权重
- 词汇重叠相似度：基于词汇重叠度计算
- 延迟导入NumPy，减少启动时间

#### 3.3.4 文件处理工具模块 (`src/file_utils.py`)

**核心函数接口：**
```python
def read_file(file_path):
    """读取文件内容"""
    
def write_result(file_path, original_file, plagiarized_file, similarity):
    """写入结果到文件"""
    
def generate_output_filename(original_file, plagiarized_file, output_file):
    """生成带时间戳的输出文件名"""
```

---

## 4. 模块性能分析与改进

### 4.1 性能瓶颈分析

基于cProfile性能分析结果，当前系统存在以下主要性能瓶颈：

#### 4.1.1 主要性能瓶颈

**1. jieba分词库初始化瓶颈（47.3% 总时间）**
- **问题描述**：jieba模型加载占用0.368秒，占总执行时间的47.3%
- **具体表现**：
  - `marshal.load`：0.366秒（模型文件反序列化）
  - `jieba.initialize`：0.368秒（模型初始化）
  - `jieba.check_initialized`：0.368秒（初始化检查）
- **影响**：每次程序启动都需要重新加载模型，无法复用

**2. jieba分词处理瓶颈（35.2% 总时间）**
- **问题描述**：分词处理占用0.272秒，占总执行时间的35.2%
- **具体表现**：
  - `jieba.lcut`：0.589秒（分词主函数）
  - `jieba.cut`：0.586秒（分词核心）
  - `jieba.__cut_DAG`：0.578秒（DAG分词算法）
  - `jieba.get_DAG`：0.404秒（DAG构建）
- **影响**：分词是文本处理的核心步骤，性能直接影响整体速度

**3. 模块导入开销（22.5% 总时间）**
- **问题描述**：模块导入占用0.174秒，占总执行时间的22.5%
- **具体表现**：
  - 各种模块的`__import__`和`exec_module`调用
  - 特别是jieba相关模块的导入开销较大
- **影响**：程序启动时间较长

#### 4.1.2 次要性能瓶颈

**1. 重复的文本预处理**
- **问题描述**：`vectorize_text`函数重复调用`preprocess_text`
- **具体表现**：在`algorithm.py`中，`preprocess_text`被调用2次（第31、32行）
- **影响**：造成不必要的重复计算

**2. 相似度计算效率**
- **问题描述**：当前使用简单的平均融合方式
- **具体表现**：三种相似度算法（余弦、Jaccard、词汇重叠）计算后简单平均
- **影响**：没有根据算法特性进行优化

### 4.2 性能优化策略

#### 4.2.1 jieba分词优化策略

**策略：模型预加载与缓存**
```python
# 优化方案：在模块级别预加载jieba模型
import jieba
# 在模块导入时预加载，避免运行时加载
jieba.initialize()

# 对相同文本的分词结果进行缓存
from functools import lru_cache

@lru_cache(maxsize=128)
def cached_tokenize(text):
    return jieba.lcut(text)
```

#### 4.2.2 文本预处理优化策略

**策略：并行化文本处理**
```python
# 使用多进程并行处理两个文本
from multiprocessing import Pool

def parallel_preprocess(texts):
    with Pool(2) as pool:
        results = pool.map(preprocess_text, texts)
    return results
```

#### 4.2.3 相似度计算优化策略

**策略：向量计算优化**
```python
# 使用NumPy加速向量运算
import numpy as np

def optimized_cosine_similarity(vec1, vec2):
    # 转换为NumPy数组进行快速计算
    all_words = list(set(vec1.keys()) | set(vec2.keys()))
    v1 = np.array([vec1.get(word, 0) for word in all_words])
    v2 = np.array([vec2.get(word, 0) for word in all_words])
    
    # 使用NumPy的向量化运算
    dot_product = np.dot(v1, v2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    
    return dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0
```

#### 4.2.4 内存使用优化策略

**策略：流式处理大文件**
```python
# 对于大文件，使用流式处理避免内存溢出
def process_large_file(file_path, chunk_size=10000):
    with open(file_path, 'r', encoding='utf-8') as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            yield preprocess_text(chunk)
```

#### 4.2.5 模块导入优化策略

**策略：延迟导入与模块缓存**
```python
# 延迟导入重量级模块，避免启动时加载
def get_jieba():
    import jieba
    return jieba

def get_numpy():
    import numpy as np
    return np

# 模块级缓存，避免重复导入
_imported_modules = {}

def lazy_import(module_name):
    if module_name not in _imported_modules:
        _imported_modules[module_name] = __import__(module_name)
    return _imported_modules[module_name]

# 在函数内部按需导入，减少启动时间
def calculate_similarity(text1, text2):
    # 只在需要时才导入jieba
    jieba = get_jieba()
    
```
